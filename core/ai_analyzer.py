"""
AI åˆ†ææ¨¡å—
æ”¯æŒå¤šç§ AI åç«¯è¿›è¡Œæ‰«ææ—¥å¿—åˆ†æå’Œå‘½ä»¤æ¨è
"""

import json
import requests
from enum import Enum
from typing import Optional, Dict, Any, Callable
from dataclasses import dataclass


class AIProvider(Enum):
    """AI æœåŠ¡æä¾›å•†"""
    OLLAMA = "ollama"           # æœ¬åœ° Ollama
    OPENAI = "openai"           # OpenAI
    CLAUDE = "claude"           # Anthropic Claude
    DEEPSEEK = "deepseek"       # DeepSeekï¼ˆå›½å†…æ¨èï¼‰
    QWEN = "qwen"               # é˜¿é‡Œé€šä¹‰åƒé—®
    ZHIPU = "zhipu"             # æ™ºè°± GLM
    MOONSHOT = "moonshot"       # æœˆä¹‹æš—é¢ Kimi
    CUSTOM = "custom"           # è‡ªå®šä¹‰ API


# AI æœåŠ¡é¢„è®¾é…ç½®
AI_PROVIDER_PRESETS = {
    AIProvider.OLLAMA: {
        "name": "Ollamaï¼ˆæœ¬åœ°ï¼‰",
        "base_url": "http://localhost:11434",
        "default_model": "qwen2:7b",
        "is_local": True,
    },
    AIProvider.OPENAI: {
        "name": "OpenAI",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-4o-mini",
        "is_local": False,
    },
    AIProvider.CLAUDE: {
        "name": "Claude",
        "base_url": "https://api.anthropic.com",
        "default_model": "claude-3-haiku-20240307",
        "is_local": False,
    },
    AIProvider.DEEPSEEK: {
        "name": "DeepSeek",
        "base_url": "https://api.deepseek.com",
        "default_model": "deepseek-chat",
        "is_local": False,
    },
    AIProvider.QWEN: {
        "name": "é˜¿é‡Œé€šä¹‰åƒé—®",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "default_model": "qwen-turbo",
        "is_local": False,
    },
    AIProvider.ZHIPU: {
        "name": "æ™ºè°±GLM",
        "base_url": "https://open.bigmodel.cn/api/paas/v4",
        "default_model": "glm-4-flash",
        "is_local": False,
    },
    AIProvider.MOONSHOT: {
        "name": "æœˆä¹‹æš—é¢Kimi",
        "base_url": "https://api.moonshot.cn/v1",
        "default_model": "moonshot-v1-8k",
        "is_local": False,
    },
    AIProvider.CUSTOM: {
        "name": "è‡ªå®šä¹‰ API",
        "base_url": "",
        "default_model": "",
        "is_local": False,
    },
}


@dataclass
class AIConfig:
    """AI é…ç½®"""
    provider: AIProvider
    api_key: str = ""
    base_url: str = ""
    model: str = ""
    max_tokens: int = 2000
    temperature: float = 0.7
    timeout: int = 60


@dataclass
class AIResponse:
    """AI å“åº”ç»“æœ"""
    success: bool
    content: str = ""
    error: str = ""
    usage: Dict[str, int] = None
    
    def __post_init__(self):
        if self.usage is None:
            self.usage = {}


class AIAnalyzer:
    """AI åˆ†æå™¨ - ç»Ÿä¸€çš„ AI åˆ†ææ¥å£"""
    
    # ç³»ç»Ÿæç¤ºè¯ - SQL æ³¨å…¥åˆ†æä¸“å®¶
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ SQL æ³¨å…¥å®‰å…¨æµ‹è¯•ä¸“å®¶ï¼Œç²¾é€š SQLMap å·¥å…·çš„ä½¿ç”¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æ SQLMap çš„æ‰«ææ—¥å¿—ï¼Œå¹¶æä¾›ä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ã€‚

ä½ çš„å›ç­”åº”è¯¥ï¼š
1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
2. ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨æ ‡é¢˜å’Œåˆ—è¡¨
3. é’ˆå¯¹å®‰å…¨æµ‹è¯•åœºæ™¯ï¼Œæä¾›å®ç”¨å»ºè®®
4. å¦‚æœæ¨èå‘½ä»¤å‚æ•°ï¼Œè¯·ç»™å‡ºå®Œæ•´çš„å‚æ•°æ ¼å¼"""
    
    def __init__(self, config: AIConfig):
        """
        åˆå§‹åŒ– AI åˆ†æå™¨
        
        å‚æ•°:
            config: AI é…ç½®å¯¹è±¡
        """
        self.config = config
        self._set_default_url_if_needed()
    
    def _set_default_url_if_needed(self):
        """å¦‚æœæœªè®¾ç½® base_urlï¼Œä½¿ç”¨é¢„è®¾å€¼"""
        if not self.config.base_url and self.config.provider in AI_PROVIDER_PRESETS:
            self.config.base_url = AI_PROVIDER_PRESETS[self.config.provider]["base_url"]
        if not self.config.model and self.config.provider in AI_PROVIDER_PRESETS:
            self.config.model = AI_PROVIDER_PRESETS[self.config.provider]["default_model"]
    
    def test_connection(self) -> AIResponse:
        """
        æµ‹è¯• AI æœåŠ¡è¿æ¥
        
        è¿”å›:
            AIResponse: åŒ…å«æµ‹è¯•ç»“æœ
        """
        try:
            if self.config.provider == AIProvider.OLLAMA:
                return self._test_ollama_connection()
            elif self.config.provider == AIProvider.CLAUDE:
                return self._test_claude_connection()
            else:
                # OpenAI å…¼å®¹æ¥å£ï¼ˆåŒ…æ‹¬å›½å†…æœåŠ¡ï¼‰
                return self._test_openai_compatible_connection()
        except requests.exceptions.Timeout:
            return AIResponse(success=False, error="è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡åœ°å€")
        except requests.exceptions.ConnectionError:
            return AIResponse(success=False, error="æ— æ³•è¿æ¥åˆ°æœåŠ¡ï¼Œè¯·æ£€æŸ¥æœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡®")
        except Exception as e:
            return AIResponse(success=False, error=f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
    
    def _test_ollama_connection(self) -> AIResponse:
        """æµ‹è¯• Ollama è¿æ¥"""
        url = f"{self.config.base_url.rstrip('/')}/api/tags"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m.get("name", "") for m in models]
            if self.config.model in model_names or any(self.config.model in m for m in model_names):
                return AIResponse(success=True, content=f"è¿æ¥æˆåŠŸï¼å·²æ‰¾åˆ°æ¨¡å‹: {self.config.model}")
            else:
                available = ", ".join(model_names[:5]) if model_names else "æ— "
                return AIResponse(
                    success=False, 
                    error=f"æ¨¡å‹ {self.config.model} æœªæ‰¾åˆ°ã€‚å¯ç”¨æ¨¡å‹: {available}"
                )
        return AIResponse(success=False, error=f"æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
    
    def _test_openai_compatible_connection(self) -> AIResponse:
        """æµ‹è¯• OpenAI å…¼å®¹æ¥å£è¿æ¥"""
        url = f"{self.config.base_url.rstrip('/')}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.api_key}"
        }
        data = {
            "model": self.config.model,
            "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"}],
            "max_tokens": 20
        }
        response = requests.post(url, headers=headers, json=data, timeout=15)
        if response.status_code == 200:
            result = response.json()
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            return AIResponse(success=True, content=f"è¿æ¥æˆåŠŸï¼æ¨¡å‹å“åº”: {content[:50]}")
        else:
            error_msg = response.json().get("error", {}).get("message", response.text)
            return AIResponse(success=False, error=f"API é”™è¯¯: {error_msg}")
    
    def _test_claude_connection(self) -> AIResponse:
        """æµ‹è¯• Claude API è¿æ¥"""
        url = f"{self.config.base_url.rstrip('/')}/v1/messages"
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        data = {
            "model": self.config.model,
            "max_tokens": 20,
            "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"}]
        }
        response = requests.post(url, headers=headers, json=data, timeout=15)
        if response.status_code == 200:
            result = response.json()
            content = result.get("content", [{}])[0].get("text", "")
            return AIResponse(success=True, content=f"è¿æ¥æˆåŠŸï¼æ¨¡å‹å“åº”: {content[:50]}")
        else:
            error_msg = response.json().get("error", {}).get("message", response.text)
            return AIResponse(success=False, error=f"API é”™è¯¯: {error_msg}")
    
    def analyze_log(self, log_content: str, callback: Callable[[str], None] = None) -> AIResponse:
        """
        åˆ†ææ‰«ææ—¥å¿—
        
        å‚æ•°:
            log_content: SQLMap æ‰«ææ—¥å¿—å†…å®¹
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            AIResponse: åˆ†æç»“æœ
        """
        if not log_content or not log_content.strip():
            return AIResponse(success=False, error="æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
        
        # é™åˆ¶æ—¥å¿—é•¿åº¦ï¼Œé¿å…è¶…å‡º token é™åˆ¶
        max_log_length = 8000
        if len(log_content) > max_log_length:
            # ä¿ç•™å¼€å¤´å’Œç»“å°¾éƒ¨åˆ†
            half = max_log_length // 2
            log_content = log_content[:half] + "\n\n... [æ—¥å¿—è¿‡é•¿ï¼Œå·²æˆªæ–­] ...\n\n" + log_content[-half:]
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ SQLMap æ‰«ææ—¥å¿—ï¼Œå¹¶æä¾›ï¼š

## åˆ†æè¦æ±‚
1. **æ‰«æçŠ¶æ€æ€»ç»“**ï¼šå½“å‰æ‰«æè¿›åº¦ã€æ˜¯å¦å‘ç°æ³¨å…¥ç‚¹
2. **æ³¨å…¥ç‚¹ä¿¡æ¯**ï¼šå¦‚å‘ç°æ³¨å…¥ï¼Œåˆ—å‡ºæ³¨å…¥ç±»å‹ã€å‚æ•°ã€Payload
3. **æ•°æ®åº“ä¿¡æ¯**ï¼šå¦‚å·²è·å–ï¼Œåˆ—å‡ºæ•°æ®åº“ç±»å‹ã€ç‰ˆæœ¬ã€å½“å‰æ•°æ®åº“ç­‰
4. **é—®é¢˜è¯Šæ–­**ï¼šå¦‚æœ‰é”™è¯¯æˆ–è­¦å‘Šï¼Œåˆ†æå¯èƒ½åŸå› 
5. **ä¼˜åŒ–å»ºè®®**ï¼šå¦‚ä½•æé«˜æ‰«ææ•ˆç‡æˆ–æˆåŠŸç‡

## æ‰«ææ—¥å¿—
```
{log_content}
```

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„æ¸…æ™°ã€‚"""
        
        return self._send_request(prompt, callback)
    
    def suggest_command(self, log_content: str, current_command: str = "", callback: Callable[[str], None] = None) -> AIResponse:
        """
        æ ¹æ®æ—¥å¿—æ¨èä¼˜åŒ–å‘½ä»¤
        
        å‚æ•°:
            log_content: SQLMap æ‰«ææ—¥å¿—å†…å®¹
            current_command: å½“å‰ä½¿ç”¨çš„å‘½ä»¤ï¼ˆå¯é€‰ï¼‰
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            AIResponse: å‘½ä»¤å»ºè®®
        """
        if not log_content or not log_content.strip():
            return AIResponse(success=False, error="æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå»ºè®®")
        
        # é™åˆ¶æ—¥å¿—é•¿åº¦
        max_log_length = 6000
        if len(log_content) > max_log_length:
            log_content = log_content[-max_log_length:]  # ä¿ç•™æœ€æ–°çš„æ—¥å¿—
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ SQLMap æ‰«ææ—¥å¿—ï¼Œæ¨èæ›´ä¼˜çš„æ‰«æå‚æ•°ã€‚

## å½“å‰å‘½ä»¤
```
{current_command if current_command else "æœªæä¾›"}
```

## æœ€è¿‘æ‰«ææ—¥å¿—
```
{log_content}
```

## è¯·æä¾›

### 1. é—®é¢˜åˆ†æ
ç®€è¦åˆ†æå½“å‰æ‰«æé‡åˆ°çš„é—®é¢˜æˆ–å¯ä¼˜åŒ–çš„åœ°æ–¹ã€‚

### 2. æ¨èå‚æ•°
æ ¹æ®åˆ†æç»“æœï¼Œæ¨èå…·ä½“çš„ SQLMap å‚æ•°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| --tamper | è„šæœ¬å | æ¨èåŸå›  |
| --technique | æŠ€æœ¯ç±»å‹ | æ¨èåŸå›  |
| ... | ... | ... |

### 3. å®Œæ•´æ¨èå‘½ä»¤
ç»™å‡ºå¯ç›´æ¥ä½¿ç”¨çš„å®Œæ•´å‘½ä»¤è¡Œï¼ˆä¸åŒ…å« URLï¼Œåªç»™å‚æ•°éƒ¨åˆ†ï¼‰ã€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"""
        
        return self._send_request(prompt, callback)
    
    def diagnose_error(self, log_content: str, error_message: str = "", callback: Callable[[str], None] = None) -> AIResponse:
        """
        è¯Šæ–­æ‰«æé”™è¯¯
        
        å‚æ•°:
            log_content: SQLMap æ‰«ææ—¥å¿—å†…å®¹
            error_message: å…·ä½“é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            AIResponse: è¯Šæ–­ç»“æœå’Œè§£å†³æ–¹æ¡ˆ
        """
        prompt = f"""è¯·è¯Šæ–­ä»¥ä¸‹ SQLMap æ‰«æä¸­é‡åˆ°çš„é—®é¢˜ã€‚

## é”™è¯¯ä¿¡æ¯
```
{error_message if error_message else "æœªæä¾›å…·ä½“é”™è¯¯ä¿¡æ¯"}
```

## æ‰«ææ—¥å¿—
```
{log_content[-4000:] if len(log_content) > 4000 else log_content}
```

## è¯·åˆ†æ

### 1. é—®é¢˜åŸå› 
åˆ†æå¯¼è‡´é—®é¢˜çš„å¯èƒ½åŸå› ã€‚

### 2. è§£å†³æ–¹æ¡ˆ
ç»™å‡ºå…·ä½“çš„è§£å†³æ­¥éª¤ã€‚

### 3. å‚æ•°è°ƒæ•´
å¦‚éœ€è°ƒæ•´å‚æ•°ï¼Œç»™å‡ºå…·ä½“å»ºè®®ã€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæä¾›å¯æ“ä½œçš„è§£å†³æ–¹æ¡ˆã€‚"""
        
        return self._send_request(prompt, callback)
    
    def analyze_and_suggest(self, log_content: str, current_command: str = "", callback: Callable[[str], None] = None) -> AIResponse:
        """
        æ•´åˆåˆ†æï¼šåˆ†ææ—¥å¿—å¹¶æ¨èä¼˜åŒ–å‚æ•°
        
        å‚æ•°:
            log_content: SQLMap æ‰«ææ—¥å¿—å†…å®¹
            current_command: å½“å‰ä½¿ç”¨çš„å‘½ä»¤ï¼ˆå¯é€‰ï¼‰
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            AIResponse: åˆ†æç»“æœå’Œæ¨èå‚æ•°
        """
        if not log_content or not log_content.strip():
            return AIResponse(success=False, error="æ—¥å¿—å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
        
        # é™åˆ¶æ—¥å¿—é•¿åº¦
        max_log_length = 6000
        if len(log_content) > max_log_length:
            # ä¿ç•™å¼€å¤´å’Œç»“å°¾éƒ¨åˆ†
            head = max_log_length // 3
            tail = max_log_length * 2 // 3
            log_content = log_content[:head] + "\n\n... [æ—¥å¿—è¿‡é•¿ï¼Œå·²æˆªæ–­] ...\n\n" + log_content[-tail:]
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ SQLMap æ‰«ææ—¥å¿—ï¼Œç»™å‡º**ä¸“ä¸šçš„åˆ†æç»“æœ**ã€‚

## å½“å‰å‘½ä»¤
```
{current_command if current_command else "æœªæä¾›"}
```

## æ‰«ææ—¥å¿—
```
{log_content}
```

## åˆ†æè§„åˆ™
**è¯·å…ˆåˆ¤æ–­æ—¥å¿—ä¸­æ˜¯å¦å·²ç»æˆåŠŸå‘ç° SQL æ³¨å…¥æ¼æ´ã€‚**

### æƒ…å†µä¸€ï¼šå¦‚æœå·²å‘ç°æ³¨å…¥æ¼æ´ï¼ˆæ—¥å¿—ä¸­æœ‰ "is vulnerable"ã€"identified" ç­‰æˆåŠŸæ ‡å¿—ï¼‰
åªéœ€è¦ç»™å‡ºä»¥ä¸‹å†…å®¹ï¼Œ**ä¸éœ€è¦ç»™å‡ºæ‰«ææ–¹æ¡ˆ**ï¼š

#### âœ… æ¼æ´å‘ç°æŠ¥å‘Š

**1. æ³¨å…¥ç‚¹ä¿¡æ¯**
- æ¼æ´å‚æ•°ï¼š
- æ³¨å…¥ç±»å‹ï¼šï¼ˆå¸ƒå°”ç›²æ³¨/æŠ¥é”™æ³¨å…¥/è”åˆæŸ¥è¯¢/æ—¶é—´ç›²æ³¨ç­‰ï¼‰
- æ•°æ®åº“ç±»å‹ï¼š

**2. æˆåŠŸçš„ Payload**
å®Œæ•´åˆ—å‡ºæ—¥å¿—ä¸­æ‰€æœ‰æˆåŠŸçš„æ³¨å…¥ payloadï¼Œæ ¼å¼å¦‚ï¼š
```
Payload 1: xxx
Payload 2: xxx
```

**3. å·²è·å–çš„ä¿¡æ¯**
- æ•°æ®åº“åï¼š
- æ•°æ®åº“ç‰ˆæœ¬ï¼š
- å½“å‰ç”¨æˆ·ï¼š
- å…¶ä»–ä¿¡æ¯ï¼š

**4. åç»­å»ºè®®**
ç®€è¦è¯´æ˜ç”¨æˆ·å¯ä»¥è¿›è¡Œçš„ä¸‹ä¸€æ­¥æ“ä½œï¼ˆå¦‚æšä¸¾è¡¨ã€æå–æ•°æ®ç­‰ï¼‰

---

### æƒ…å†µäºŒï¼šå¦‚æœæœªå‘ç°æ³¨å…¥æ¼æ´æˆ–æ‰«æé‡åˆ°é—®é¢˜
ç»™å‡ºä»¥ä¸‹å†…å®¹ï¼š

#### âŒ æ‰«æçŠ¶æ€åˆ†æ

**1. å½“å‰çŠ¶æ€**
- æ‰«æè¿›åº¦
- é‡åˆ°çš„é—®é¢˜ï¼ˆWAFã€æ— æ³¨å…¥ç‚¹ã€è¿æ¥å¤±è´¥ç­‰ï¼‰
- é—®é¢˜åŸå› åˆ†æ

**2. å®‰å…¨æ–¹æ¡ˆï¼ˆğŸŸ¢ æ¨èï¼‰**
ä¿å®ˆç¨³å®šçš„ä¼˜åŒ–å»ºè®®ï¼Œåªå…³æ³¨æ¼æ´æ£€æµ‹ï¼š
- risk ä¿æŒ 1
- level æ§åˆ¶åœ¨ 1-3
- ä¸æ¨è os-shellã€dump-all ç­‰é«˜çº§åŠŸèƒ½

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| --tamper | è„šæœ¬å | ç»•è¿‡è¿‡æ»¤ |
| --level | 1-3 | é€‚åº¦æ¢æµ‹ |
| --risk | 1 | ä¿æŒå®‰å…¨ |

**å®‰å…¨æ–¹æ¡ˆå‘½ä»¤ï¼š**
```
[SAFE] --tamper=xxx --level=2 --risk=1 --threads=3 --random-agent --batch
```

**3. æ¿€è¿›æ–¹æ¡ˆï¼ˆğŸ”´ è°¨æ…ï¼‰**
é«˜é£é™©æ–¹æ¡ˆï¼Œéœ€ç”¨æˆ·ç¡®è®¤ï¼š

```
[AGGRESSIVE] --level=5 --risk=3 --tamper=xxx
```

**4. ä¸“å®¶å»ºè®®**
å…¶ä»–æµ‹è¯•æ€è·¯æˆ–ç»•è¿‡æŠ€å·§

---

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚æ ¹æ®æ—¥å¿—å®é™…æƒ…å†µé€‰æ‹©ä¸Šè¿°ä¸¤ç§æ ¼å¼ä¹‹ä¸€å›å¤ã€‚"""
        
        return self._send_request(prompt, callback)
    
    def _send_request(self, prompt: str, callback: Callable[[str], None] = None) -> AIResponse:
        """
        å‘é€è¯·æ±‚åˆ° AI æœåŠ¡
        
        å‚æ•°:
            prompt: ç”¨æˆ·æç¤ºè¯
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            AIResponse: AI å“åº”ç»“æœ
        """
        try:
            if self.config.provider == AIProvider.OLLAMA:
                return self._send_ollama_request(prompt, callback)
            elif self.config.provider == AIProvider.CLAUDE:
                return self._send_claude_request(prompt, callback)
            else:
                # OpenAI å…¼å®¹æ¥å£ï¼ˆåŒ…æ‹¬å›½å†…æœåŠ¡ï¼‰
                return self._send_openai_compatible_request(prompt, callback)
        except requests.exceptions.Timeout:
            return AIResponse(success=False, error="è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        except requests.exceptions.ConnectionError:
            return AIResponse(success=False, error="æ— æ³•è¿æ¥åˆ° AI æœåŠ¡ï¼Œè¯·æ£€æŸ¥æœåŠ¡åœ°å€å’Œç½‘ç»œ")
        except Exception as e:
            return AIResponse(success=False, error=f"è¯·æ±‚å¤±è´¥: {str(e)}")
    
    def _send_ollama_request(self, prompt: str, callback: Callable[[str], None] = None) -> AIResponse:
        """å‘é€ Ollama è¯·æ±‚"""
        url = f"{self.config.base_url.rstrip('/')}/api/generate"
        data = {
            "model": self.config.model,
            "prompt": f"{self.SYSTEM_PROMPT}\n\nç”¨æˆ·é—®é¢˜ï¼š\n{prompt}",
            "stream": callback is not None,
            "options": {
                "temperature": self.config.temperature,
                "num_predict": self.config.max_tokens,
            }
        }
        
        if callback:
            # æµå¼è¯·æ±‚
            response = requests.post(url, json=data, stream=True, timeout=self.config.timeout)
            full_content = ""
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line)
                        content = chunk.get("response", "")
                        full_content += content
                        callback(content)
                        if chunk.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
            return AIResponse(success=True, content=full_content)
        else:
            # éæµå¼è¯·æ±‚
            response = requests.post(url, json=data, timeout=self.config.timeout)
            if response.status_code == 200:
                result = response.json()
                return AIResponse(success=True, content=result.get("response", ""))
            else:
                return AIResponse(success=False, error=f"Ollama é”™è¯¯: {response.text}")
    
    def _send_openai_compatible_request(self, prompt: str, callback: Callable[[str], None] = None) -> AIResponse:
        """å‘é€ OpenAI å…¼å®¹è¯·æ±‚ï¼ˆæ”¯æŒå›½å†…æœåŠ¡ï¼‰"""
        url = f"{self.config.base_url.rstrip('/')}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.api_key}"
        }
        data = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
            "stream": callback is not None
        }
        
        if callback:
            # æµå¼è¯·æ±‚
            response = requests.post(url, headers=headers, json=data, stream=True, timeout=self.config.timeout)
            full_content = ""
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith("data: "):
                        line_text = line_text[6:]
                    if line_text == "[DONE]":
                        break
                    try:
                        chunk = json.loads(line_text)
                        delta = chunk.get("choices", [{}])[0].get("delta", {})
                        content = delta.get("content", "")
                        if content:
                            full_content += content
                            callback(content)
                    except json.JSONDecodeError:
                        continue
            return AIResponse(success=True, content=full_content)
        else:
            # éæµå¼è¯·æ±‚
            response = requests.post(url, headers=headers, json=data, timeout=self.config.timeout)
            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                usage = result.get("usage", {})
                return AIResponse(success=True, content=content, usage=usage)
            else:
                try:
                    error_msg = response.json().get("error", {}).get("message", response.text)
                except:
                    error_msg = response.text
                return AIResponse(success=False, error=f"API é”™è¯¯: {error_msg}")
    
    def _send_claude_request(self, prompt: str, callback: Callable[[str], None] = None) -> AIResponse:
        """å‘é€ Claude è¯·æ±‚"""
        url = f"{self.config.base_url.rstrip('/')}/v1/messages"
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        data = {
            "model": self.config.model,
            "max_tokens": self.config.max_tokens,
            "system": self.SYSTEM_PROMPT,
            "messages": [{"role": "user", "content": prompt}],
            "stream": callback is not None
        }
        
        if callback:
            # æµå¼è¯·æ±‚
            response = requests.post(url, headers=headers, json=data, stream=True, timeout=self.config.timeout)
            full_content = ""
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith("data: "):
                        line_text = line_text[6:]
                    try:
                        chunk = json.loads(line_text)
                        if chunk.get("type") == "content_block_delta":
                            content = chunk.get("delta", {}).get("text", "")
                            if content:
                                full_content += content
                                callback(content)
                    except json.JSONDecodeError:
                        continue
            return AIResponse(success=True, content=full_content)
        else:
            # éæµå¼è¯·æ±‚
            response = requests.post(url, headers=headers, json=data, timeout=self.config.timeout)
            if response.status_code == 200:
                result = response.json()
                content = result.get("content", [{}])[0].get("text", "")
                usage = result.get("usage", {})
                return AIResponse(success=True, content=content, usage=usage)
            else:
                try:
                    error_msg = response.json().get("error", {}).get("message", response.text)
                except:
                    error_msg = response.text
                return AIResponse(success=False, error=f"Claude é”™è¯¯: {error_msg}")


def create_analyzer_from_config(config_manager) -> AIAnalyzer:
    """
    ä»é…ç½®ç®¡ç†å™¨åˆ›å»º AI åˆ†æå™¨
    
    å‚æ•°:
        config_manager: ConfigManager å®ä¾‹
    
    è¿”å›:
        AIAnalyzer: é…ç½®å¥½çš„åˆ†æå™¨å®ä¾‹
    """
    provider_str = config_manager.get('AI', 'provider', 'ollama')
    
    try:
        provider = AIProvider(provider_str)
    except ValueError:
        provider = AIProvider.OLLAMA
    
    # æ ¹æ®ä¸åŒçš„ provider è¯»å–å¯¹åº”é…ç½®
    if provider == AIProvider.OLLAMA:
        base_url = config_manager.get('AI', 'ollama_url', 'http://localhost:11434')
        model = config_manager.get('AI', 'ollama_model', 'qwen2:7b')
        api_key = ""
    elif provider == AIProvider.OPENAI:
        base_url = config_manager.get('AI', 'openai_base_url', 'https://api.openai.com/v1')
        model = config_manager.get('AI', 'openai_model', 'gpt-4o-mini')
        api_key = config_manager.get('AI', 'openai_api_key', '')
    elif provider == AIProvider.CLAUDE:
        base_url = config_manager.get('AI', 'claude_base_url', 'https://api.anthropic.com')
        model = config_manager.get('AI', 'claude_model', 'claude-3-haiku-20240307')
        api_key = config_manager.get('AI', 'claude_api_key', '')
    elif provider == AIProvider.DEEPSEEK:
        base_url = "https://api.deepseek.com"
        model = config_manager.get('AI', 'deepseek_model', 'deepseek-chat')
        api_key = config_manager.get('AI', 'deepseek_api_key', '')
    elif provider == AIProvider.QWEN:
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        model = config_manager.get('AI', 'qwen_model', 'qwen-turbo')
        api_key = config_manager.get('AI', 'qwen_api_key', '')
    elif provider == AIProvider.ZHIPU:
        base_url = "https://open.bigmodel.cn/api/paas/v4"
        model = config_manager.get('AI', 'zhipu_model', 'glm-4-flash')
        api_key = config_manager.get('AI', 'zhipu_api_key', '')
    elif provider == AIProvider.MOONSHOT:
        base_url = "https://api.moonshot.cn/v1"
        model = config_manager.get('AI', 'moonshot_model', 'moonshot-v1-8k')
        api_key = config_manager.get('AI', 'moonshot_api_key', '')
    else:  # CUSTOM
        base_url = config_manager.get('AI', 'custom_base_url', '')
        model = config_manager.get('AI', 'custom_model', '')
        api_key = config_manager.get('AI', 'custom_api_key', '')
    
    config = AIConfig(
        provider=provider,
        api_key=api_key,
        base_url=base_url,
        model=model,
        max_tokens=config_manager.get_int('AI', 'max_tokens', 2000),
        temperature=config_manager.get_float('AI', 'temperature', 0.7),
        timeout=config_manager.get_int('AI', 'timeout', 60)
    )
    
    return AIAnalyzer(config)
